{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['element'] ['an american troop increase'] \n",
      "\n",
      "['mistreatment'] ['sexual abuse'] \n",
      "\n",
      "['wild animal'] ['president', 'candidate', 'donor', 'activist', 'alligator'] \n",
      "\n",
      "['market observer'] ['he'] \n",
      "\n",
      "['officer'] ['he'] \n",
      "\n",
      "['blogger'] ['he'] \n",
      "\n",
      "['item'] ['material', 'furniture'] \n",
      "\n",
      "['unauthorized absence'] ['desertion'] \n",
      "\n",
      "['piece'] ['32 painting'] \n",
      "\n",
      "['director'] ['she'] \n",
      "\n",
      "['large airport'] ['la guardia airport'] \n",
      "\n",
      "['online video site'] ['youtube'] \n",
      "\n",
      "['gift'] ['vacation trip'] \n",
      "\n",
      "['payment'] ['fine'] \n",
      "\n",
      "['favor'] ['gold coin'] \n",
      "\n",
      "['artifact'] ['gold coin'] \n",
      "\n",
      "['top aide'] ['karl rove'] \n",
      "\n",
      "['agency'] ['the white house'] \n",
      "\n",
      "['envoy'] ['monday'] \n",
      "\n",
      "['gas'] ['carbon dioxide'] \n",
      "\n",
      "['profession'] ['politic', 'law', 'the news medium'] \n",
      "\n",
      "['dangerous item'] ['bomb', 'chemical'] \n",
      "\n",
      "['government agency'] ['the president', 'the prime minister', 'parliament'] \n",
      "\n",
      "['protection'] ['a strong real estate market'] \n",
      "\n",
      "['financial firm'] ['investment company'] \n",
      "\n",
      "['offense'] ['justice'] \n",
      "\n",
      "['agricultural pollutant'] ['manure'] \n",
      "\n",
      "['training ground'] ['the police academy', 'the firing range'] \n",
      "\n",
      "['line'] ['credit card', 'home equity loan'] \n",
      "\n",
      "['income  produce property'] ['office building', 'hotel'] \n",
      "\n",
      "['monument he'] ['hubristic statue', 'mural', 'fresco'] \n",
      "\n",
      "['practice they'] ['abortion', 'embryonic stem cell research'] \n",
      "\n",
      "['crime'] ['duty'] \n",
      "\n",
      "['industry professional'] ['japanese engineer'] \n",
      "\n",
      "['company'] ['image collection'] \n",
      "\n",
      "['detainee'] ['he'] \n",
      "\n",
      "['bank  sponsor function'] ['client'] \n",
      "\n",
      "['railroad component'] ['the tank engine wooden train'] \n",
      "\n",
      "['communication'] ['telephone call'] \n",
      "\n",
      "['politician'] ['presidential candidate'] \n",
      "\n",
      "['attribute'] ['job skill', 'education'] \n",
      "\n",
      "['content'] ['video', 'image', 'map'] \n",
      "\n",
      "['critic'] ['academic'] \n",
      "\n",
      "['big company'] ['wal  mart'] \n",
      "\n",
      "['incriminate evidence'] ['their weapon'] \n",
      "\n",
      "['incentive'] ['rebate'] \n",
      "\n",
      "['fabric'] ['wool'] \n",
      "\n",
      "['disease'] ['aid'] \n",
      "\n",
      "['greenhouse gas'] ['carbon dioxide'] \n",
      "\n",
      "['asset'] ['the glass and steel yukos headquarters building'] \n",
      "\n",
      "['symptom'] ['hallucination'] \n",
      "\n",
      "['route'] ['the n6'] \n",
      "\n",
      "['problem'] ['delay'] \n",
      "\n",
      "['republican  inspire change'] ['abortion'] \n",
      "\n",
      "['prominent medication'] ['vioxx', 'antidepressant'] \n",
      "\n",
      "['partsif they'] ['falluja'] \n",
      "\n",
      "['essential commodity'] ['iron', 'copper'] \n",
      "\n",
      "['animal'] ['the doe'] \n",
      "\n",
      "['sanction'] ['asset'] \n",
      "\n",
      "['possession'] ['customer  clothing'] \n",
      "\n",
      "['privacy violation'] ['identity theft'] \n",
      "\n",
      "['company'] ['google', 'yahoo'] \n",
      "\n",
      "['environmental change'] ['drought', 'rainfall'] \n",
      "\n",
      "['matter'] ['favoritism'] \n",
      "\n",
      "['aid'] ['financial gift'] \n",
      "\n",
      "['develop country'] ['china'] \n",
      "\n",
      "['abuse'] ['kidnapping', 'torture'] \n",
      "\n",
      "['minority group'] ['black'] \n",
      "\n",
      "['criterion'] ['race'] \n",
      "\n",
      "['corporate executive who'] ['grasso', 'maurice greenberg'] \n",
      "\n",
      "['peril'] ['wound'] \n",
      "\n",
      "['european leader'] ['britain'] \n",
      "\n",
      "['high  rank white house official'] ['cabinet secretary'] \n",
      "\n",
      "['official'] ['cieslewicz'] \n",
      "\n",
      "['official'] ['their fellow democrat', 'lawmaker'] \n",
      "\n",
      "['type'] ['mortgage'] \n",
      "\n",
      "['asian nation'] ['china', 'india'] \n",
      "\n",
      "['vehicle'] ['california', 'pickup truck'] \n",
      "\n",
      "['toxic hydrocarbon gas'] ['cancer  cause benzene'] \n",
      "\n",
      "['foreign oil company'] ['shell'] \n",
      "\n",
      "['dietary supplement'] ['herb', 'vitamin'] \n",
      "\n",
      "['large airport'] ['la guardia airport'] \n",
      "\n",
      "['advertising'] ['television', 'print'] \n",
      "\n",
      "['democraticpresident bush'] ['he'] \n",
      "\n",
      "['top aide'] ['karl rove'] \n",
      "\n",
      "['domestic injury case'] ['tobacco', 'asbestos'] \n",
      "\n",
      "['sexual offense'] ['rape'] \n",
      "\n",
      "['global  warm gas'] ['carbon dioxide'] \n",
      "\n",
      "['life science'] ['stem cell research'] \n",
      "\n",
      "['benefit'] ['member low  cost health coverage'] \n",
      "\n",
      "['major scene'] ['rap battle'] \n",
      "\n",
      "['entertainment company'] ['the medium world', 'hollywood studio'] \n",
      "\n",
      "['nearby state'] ['illinois', 'iowa'] \n",
      "\n",
      "['violent crime'] ['two separate homicide'] \n",
      "\n",
      "['cause'] ['heart disease'] \n",
      "\n",
      "['coastal state'] ['florida'] \n",
      "\n",
      "['homegrown nonprofit group'] ['community  development corporation'] \n",
      "\n",
      "['destructive action'] ['arson'] \n",
      "\n",
      "Count is  131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('an american troop increase', 'element'),\n",
       " ('sexual abuse', 'mistreatment'),\n",
       " ('president', 'wild animal'),\n",
       " ('candidate', 'wild animal'),\n",
       " ('donor', 'wild animal'),\n",
       " ('activist', 'wild animal'),\n",
       " ('alligator', 'wild animal'),\n",
       " ('he', 'market observer'),\n",
       " ('he', 'officer'),\n",
       " ('he', 'blogger'),\n",
       " ('material', 'item'),\n",
       " ('furniture', 'item'),\n",
       " ('desertion', 'unauthorized absence'),\n",
       " ('32 painting', 'piece'),\n",
       " ('she', 'director'),\n",
       " ('la guardia airport', 'large airport'),\n",
       " ('youtube', 'online video site'),\n",
       " ('vacation trip', 'gift'),\n",
       " ('fine', 'payment'),\n",
       " ('gold coin', 'favor'),\n",
       " ('gold coin', 'artifact'),\n",
       " ('karl rove', 'top aide'),\n",
       " ('the white house', 'agency'),\n",
       " ('monday', 'envoy'),\n",
       " ('carbon dioxide', 'gas'),\n",
       " ('politic', 'profession'),\n",
       " ('law', 'profession'),\n",
       " ('the news medium', 'profession'),\n",
       " ('bomb', 'dangerous item'),\n",
       " ('chemical', 'dangerous item'),\n",
       " ('the president', 'government agency'),\n",
       " ('the prime minister', 'government agency'),\n",
       " ('parliament', 'government agency'),\n",
       " ('a strong real estate market', 'protection'),\n",
       " ('investment company', 'financial firm'),\n",
       " ('justice', 'offense'),\n",
       " ('manure', 'agricultural pollutant'),\n",
       " ('the police academy', 'training ground'),\n",
       " ('the firing range', 'training ground'),\n",
       " ('credit card', 'line'),\n",
       " ('home equity loan', 'line'),\n",
       " ('office building', 'income  produce property'),\n",
       " ('hotel', 'income  produce property'),\n",
       " ('hubristic statue', 'monument he'),\n",
       " ('mural', 'monument he'),\n",
       " ('fresco', 'monument he'),\n",
       " ('abortion', 'practice they'),\n",
       " ('embryonic stem cell research', 'practice they'),\n",
       " ('duty', 'crime'),\n",
       " ('japanese engineer', 'industry professional'),\n",
       " ('image collection', 'company'),\n",
       " ('he', 'detainee'),\n",
       " ('client', 'bank  sponsor function'),\n",
       " ('the tank engine wooden train', 'railroad component'),\n",
       " ('telephone call', 'communication'),\n",
       " ('presidential candidate', 'politician'),\n",
       " ('job skill', 'attribute'),\n",
       " ('education', 'attribute'),\n",
       " ('video', 'content'),\n",
       " ('image', 'content'),\n",
       " ('map', 'content'),\n",
       " ('academic', 'critic'),\n",
       " ('wal  mart', 'big company'),\n",
       " ('their weapon', 'incriminate evidence'),\n",
       " ('rebate', 'incentive'),\n",
       " ('wool', 'fabric'),\n",
       " ('aid', 'disease'),\n",
       " ('carbon dioxide', 'greenhouse gas'),\n",
       " ('the glass and steel yukos headquarters building', 'asset'),\n",
       " ('hallucination', 'symptom'),\n",
       " ('the n6', 'route'),\n",
       " ('delay', 'problem'),\n",
       " ('abortion', 'republican  inspire change'),\n",
       " ('vioxx', 'prominent medication'),\n",
       " ('antidepressant', 'prominent medication'),\n",
       " ('falluja', 'partsif they'),\n",
       " ('iron', 'essential commodity'),\n",
       " ('copper', 'essential commodity'),\n",
       " ('the doe', 'animal'),\n",
       " ('asset', 'sanction'),\n",
       " ('customer  clothing', 'possession'),\n",
       " ('identity theft', 'privacy violation'),\n",
       " ('google', 'company'),\n",
       " ('yahoo', 'company'),\n",
       " ('drought', 'environmental change'),\n",
       " ('rainfall', 'environmental change'),\n",
       " ('favoritism', 'matter'),\n",
       " ('financial gift', 'aid'),\n",
       " ('china', 'develop country'),\n",
       " ('kidnapping', 'abuse'),\n",
       " ('torture', 'abuse'),\n",
       " ('black', 'minority group'),\n",
       " ('race', 'criterion'),\n",
       " ('grasso', 'corporate executive who'),\n",
       " ('maurice greenberg', 'corporate executive who'),\n",
       " ('wound', 'peril'),\n",
       " ('britain', 'european leader'),\n",
       " ('cabinet secretary', 'high  rank white house official'),\n",
       " ('cieslewicz', 'official'),\n",
       " ('their fellow democrat', 'official'),\n",
       " ('lawmaker', 'official'),\n",
       " ('mortgage', 'type'),\n",
       " ('china', 'asian nation'),\n",
       " ('india', 'asian nation'),\n",
       " ('california', 'vehicle'),\n",
       " ('pickup truck', 'vehicle'),\n",
       " ('cancer  cause benzene', 'toxic hydrocarbon gas'),\n",
       " ('shell', 'foreign oil company'),\n",
       " ('herb', 'dietary supplement'),\n",
       " ('vitamin', 'dietary supplement'),\n",
       " ('la guardia airport', 'large airport'),\n",
       " ('television', 'advertising'),\n",
       " ('print', 'advertising'),\n",
       " ('he', 'democraticpresident bush'),\n",
       " ('karl rove', 'top aide'),\n",
       " ('tobacco', 'domestic injury case'),\n",
       " ('asbestos', 'domestic injury case'),\n",
       " ('rape', 'sexual offense'),\n",
       " ('carbon dioxide', 'global  warm gas'),\n",
       " ('stem cell research', 'life science'),\n",
       " ('member low  cost health coverage', 'benefit'),\n",
       " ('rap battle', 'major scene'),\n",
       " ('the medium world', 'entertainment company'),\n",
       " ('hollywood studio', 'entertainment company'),\n",
       " ('illinois', 'nearby state'),\n",
       " ('iowa', 'nearby state'),\n",
       " ('two separate homicide', 'violent crime'),\n",
       " ('heart disease', 'cause'),\n",
       " ('florida', 'coastal state'),\n",
       " ('community  development corporation', 'homegrown nonprofit group'),\n",
       " ('arson', 'destructive action')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tag import PerceptronTagger\n",
    "import string\n",
    "import re\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "np_patterns = r\"\"\"\n",
    "                NP: {<DT|PP\\$>?<JJ>*<NN>+}\n",
    "                    {<NNP>+}\n",
    "                    {<NNS>+}\n",
    "                    \"\"\"\n",
    "\n",
    "np_chunker = nltk.RegexpParser(np_patterns)\n",
    "pos_tagger = PerceptronTagger()\n",
    "\n",
    "\n",
    "\n",
    "adj_stopwords = [\n",
    "            'able', 'available', 'brief', 'certain',\n",
    "            'different', 'due', 'enough', 'especially', 'few', 'fifth',\n",
    "            'former', 'his', 'howbeit', 'immediate', 'important', 'inc',\n",
    "            'its', 'last', 'latter', 'least', 'less', 'likely', 'little',\n",
    "            'many', 'ml', 'more', 'most', 'much', 'my', 'necessary',\n",
    "            'new', 'next', 'non', 'old', 'other', 'our', 'ours', 'own',\n",
    "            'particular', 'past', 'possible', 'present', 'proud', 'recent',\n",
    "            'same', 'several', 'significant', 'similar', 'such', 'sup', 'sure'\n",
    "        ]\n",
    "\n",
    "def chunk_spacy(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    chunks = []\n",
    "    for sentence in doc.sents:\n",
    "        sentence_text = sentence.lemma_\n",
    "        for chunk in sentence.noun_chunks:\n",
    "            if chunk.lemma_.lower() == \"example\":\n",
    "                start = chunk.start\n",
    "                pre_token = sentence[start - 1].lemma_.lower()\n",
    "                post_token = sentence[start + 1].lemma_.lower()\n",
    "                if start > 0 and\\\n",
    "                        (pre_token == \"for\" or post_token == \"of\"):\n",
    "                    continue\n",
    "            if chunk.lemma_.lower() == \"type\":\n",
    "                continue\n",
    "            chunk_arr = []\n",
    "            replace_arr = []\n",
    "            for token in chunk:\n",
    "                if token.lemma_ in adj_stopwords + [\"i.e.\", \"e.g.\"]:\n",
    "                    continue\n",
    "                chunk_arr.append(token.lemma_)\n",
    "                if token.lemma_.isalnum():\n",
    "                    replace_arr.append(token.lemma_)\n",
    "                else:\n",
    "                    replace_arr.append(''.join(\n",
    "                        char for char in token.lemma_ if char.isalnum()\n",
    "                    ))\n",
    "            if len(chunk_arr) == 0:\n",
    "                chunk_arr.append(chunk[-1].lemma_)\n",
    "            chunk_lemma = ' '.join(chunk_arr)\n",
    "            replacement_value = 'NP_' + '_'.join(replace_arr)\n",
    "            if chunk_lemma:\n",
    "                sentence_text = re.sub(r'\\b%s\\b' % re.escape(chunk_lemma),\n",
    "                                        r'%s' % replacement_value,\n",
    "                                        sentence_text)\n",
    "        chunks.append(sentence_text)\n",
    "#         print(chunks)\n",
    "    return chunks\n",
    "    \n",
    "def chunk_nltk(text):\n",
    "    sentences = nltk.sent_tokenize(text.strip()) \n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences] \n",
    "    sentences = [pos_tagger.tag(sent) for sent in sentences]\n",
    "\n",
    "    all_chunks = []\n",
    "    for sentence in sentences:\n",
    "        chunks = np_chunker.parse(sentence)\n",
    "        all_chunks.append(prepare_chunks(chunks))\n",
    "#         print(all_chunks)\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def prepare_chunks(chunks):\n",
    "    terms = []\n",
    "    for chunk in chunks:\n",
    "        label = None\n",
    "        try: \n",
    "            label = chunk.label()\n",
    "        except:\n",
    "            pass\n",
    "        if label is None: \n",
    "            token = chunk[0]\n",
    "            pos = chunk[1]\n",
    "            if pos in ['.', ':', '-', '_']:\n",
    "                continue\n",
    "            terms.append(token)\n",
    "        else:\n",
    "            np = \"NP_\"+\"_\".join([a[0] for a in chunk]) \n",
    "            terms.append(np)\n",
    "    return ' '.join(terms)\n",
    "\n",
    "def get_hyponyms(text, lib_type):\n",
    "\n",
    "    hyponyms = []\n",
    "    np_tagged_sentences = []\n",
    "    if lib_type == 'spacy':\n",
    "        np_tagged_sentences = chunk_spacy(text)\n",
    "    elif lib_type == 'nltk':\n",
    "        np_tagged_sentences = chunk_nltk(text)\n",
    "    hyponym_dict = {}\n",
    "\n",
    "    for tagged_sentence in np_tagged_sentences:\n",
    "        general_term=[]\n",
    "        specific_term=[]\n",
    "        sentence = re.sub(r\"(NP_\\w+ NP_\\w+)+\", lambda x: x.expand(r'\\1').replace(\" NP_\", \"_\"), tagged_sentence)\n",
    "        hearst_pattern =  \"((NP_\\w+ ?(, )?)+(and )?other NP_\\w+)\"  #((NP_\\w+ ?(, )?)+(and )?other NP_\\w+)\n",
    "        matches = re.search(hearst_pattern, sentence)\n",
    "        if matches:\n",
    "            match_str = matches.group(0)\n",
    "            nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "            general = nps[-1]\n",
    "            specifics = nps[:-1]\n",
    "            general_term.append(clean_term(general))\n",
    "            for i in range(len(specifics)):\n",
    "                if 'federal' in general_term:\n",
    "                    print (specifics[i], '\\n\\n')\n",
    "                specific_term.append(clean_term(specifics[i]))\n",
    "                hyponyms.append((clean_term(specifics[i]), clean_term(general)))\n",
    "            print(general_term, specific_term, '\\n')\n",
    "    return hyponyms\n",
    "\n",
    "def clean_term(term):\n",
    "    return term.replace(\"NP_\",\"\").replace(\"_\", \" \")\n",
    "    \n",
    "    \n",
    "def process_file(infile, lib_type):\n",
    "    try:\n",
    "        text = open(infile).read()\n",
    "    except:\n",
    "        print('Failed when reading file', infile, sys.exc_info()[0])\n",
    "        return [ ]\n",
    "    hyponyms = get_hyponyms(text, lib_type)\n",
    "    print('Count is ', len(hyponyms))\n",
    "    return hyponyms\n",
    "\n",
    " \n",
    "\n",
    "infile = 'and_other_sentences_NYT_sum.txt'  #and_other_sentences_NYT_sum.txt\n",
    "process_file(infile, 'spacy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
